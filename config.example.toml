# The Agency - Example Configuration File
# This file demonstrates how to configure task-specific LLM models and multiple providers

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# The Agency supports multiple LLM providers with automatic fallback
# Supported providers: Ollama, OpenAI, Azure OpenAI, Anthropic, Groq, Together AI

[llm]
# Primary provider (currently Ollama - local inference)
ollama_url = "http://localhost:11434"

# Default model for text generation
text_model = "llama3.2"

# Model for embeddings
embedding_model = "nomic-embed-text"

# Default maximum tokens for generation
max_tokens = 4096

# Default temperature for generation
temperature = 0.7

# Request timeout in seconds
timeout = 300

# Enable streaming responses
stream = false

# ============================================================================
# Multi-Provider Configuration (Optional)
# ============================================================================
# Uncomment and configure additional providers for automatic fallback

# OpenAI Configuration
# [llm.providers.openai]
# enabled = false
# api_key_env = "OPENAI_API_KEY"  # or set api_key = "sk-..." (not recommended)
# base_url = "https://api.openai.com/v1"
# text_model = "gpt-4"  # or "gpt-3.5-turbo", "gpt-4-turbo"
# embedding_model = "text-embedding-ada-002"
# max_tokens = 4096
# temperature = 0.7
# timeout = 120

# Azure OpenAI Configuration  
# [llm.providers.azure_openai]
# enabled = false
# api_key_env = "AZURE_OPENAI_API_KEY"
# endpoint = "https://your-resource.openai.azure.com"
# deployment_name = "gpt-4"
# api_version = "2023-05-15"
# max_tokens = 4096
# temperature = 0.7

# Anthropic Claude Configuration
# [llm.providers.anthropic]
# enabled = false
# api_key_env = "ANTHROPIC_API_KEY"
# base_url = "https://api.anthropic.com"
# text_model = "claude-3-opus-20240229"  # or "claude-3-sonnet-20240229"
# max_tokens = 4096
# temperature = 0.7
# timeout = 120

# Groq Configuration (Fast inference)
# [llm.providers.groq]
# enabled = false
# api_key_env = "GROQ_API_KEY"
# base_url = "https://api.groq.com/openai/v1"
# text_model = "mixtral-8x7b-32768"  # or "llama2-70b-4096"
# max_tokens = 8192
# temperature = 0.7

# Together AI Configuration
# [llm.providers.together]
# enabled = false
# api_key_env = "TOGETHER_API_KEY"
# base_url = "https://api.together.xyz/v1"
# text_model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
# max_tokens = 4096
# temperature = 0.7

# Provider Fallback Configuration
[llm.fallback]
# Enable automatic fallback to other providers when primary fails
enabled = true

# Fallback strategy: "sequential" tries providers in order, "parallel" tries all at once
strategy = "sequential"

# Maximum retry attempts per provider before moving to next
max_retries = 2

# Delay between retries in milliseconds
retry_delay_ms = 1000

# Provider priority order (highest to lowest priority)
# Only enabled providers from the list will be used
priority = ["ollama", "groq", "openai", "anthropic", "together", "azure_openai"]

# LLM response cache configuration
[llm.cache]
# Enable or disable caching
enabled = true

# Maximum number of cache entries
max_entries = 1000

# Time-to-live for cache entries in seconds (1 hour)
ttl_seconds = 3600

# SQLite database path for cache
db_path = "data/cache.db"

# Minimum temperature threshold for caching
# Only cache deterministic queries (low temperature)
min_temperature_threshold = 0.3

# Task-specific model configurations
# Each task can use a different model with custom settings

[llm.task_models.code_generation]
model = "qwen2.5-coder:7b"
max_tokens = 8192
temperature = 0.2
keywords = ["code", "program", "function", "class", "debug", "refactor", "implement"]
system_prompt = "You are an expert software engineer. Write clean, efficient, and well-documented code."

[llm.task_models.creative_writing]
model = "llama3.2:8b"
max_tokens = 4096
temperature = 0.9
keywords = ["story", "poem", "creative", "write", "narrative", "fiction"]
system_prompt = "You are a creative writer with a talent for storytelling. Be imaginative and engaging."

[llm.task_models.data_analysis]
model = "qwen2.5:7b"
max_tokens = 4096
temperature = 0.3
keywords = ["analyze", "data", "statistics", "graph", "chart", "report", "insights"]
system_prompt = "You are a data analyst. Provide clear, accurate insights based on data."

[llm.task_models.math_problem]
model = "qwen2.5:7b"
max_tokens = 2048
temperature = 0.1
keywords = ["calculate", "math", "solve", "equation", "formula", "compute"]
system_prompt = "You are a mathematics expert. Solve problems step-by-step with clear explanations."

[llm.task_models.translation]
model = "aya:8b"
max_tokens = 4096
temperature = 0.3
keywords = ["translate", "translation", "language"]
system_prompt = "You are a professional translator. Provide accurate translations while preserving context and tone."

[llm.task_models.summarization]
model = "llama3.2:3b"
max_tokens = 1024
temperature = 0.4
keywords = ["summarize", "summary", "brief", "overview", "tldr"]
system_prompt = "You are a summarization expert. Provide concise, accurate summaries of content."

[memory]
# Vector store type
store_type = "sqlite"

# Database file path
database_url = "sqlite://data/memory.db"

# Embedding dimension (must match embedding model)
embedding_dimension = 768

# Maximum number of search results
max_search_results = 10

# Similarity threshold for retrieval
similarity_threshold = 0.7

# Enable persistent storage
persistent = true

[mcp]
# Default timeout for tool calls (seconds)
default_timeout = 30

# Maximum concurrent tool calls
max_concurrent_calls = 5

# Enable tool call caching
enable_caching = true

# MCP server configurations (empty by default)
servers = {}

# Example MCP server configuration:
# [mcp.servers.example_server]
# transport = "http"
# url = "http://localhost:8000"
# enabled = true
# timeout = 30

[agent]
# Agent's name/identity
name = "The Agency AI Assistant"

# System prompt for the agent
system_prompt = """You are a helpful AI assistant with access to various tools and a memory system. \
Use your capabilities to assist users effectively. When appropriate, select the best tool or \
specialized model for the task at hand."""

# Maximum conversation history length
max_history_length = 20

# Enable memory retrieval
use_memory = true

# Enable tool calling
use_tools = true

# Maximum thinking steps for complex queries
max_thinking_steps = 5

# Enable verbose logging
verbose = false

[workflow]
# Enable workflow suspend/resume functionality
# Set to true to enable pausing and resuming workflows
enable_suspend_resume = false

# Note: Workflow snapshots are stored in the same SQLite database as memory
# (configured in [memory] section). No separate storage needed.

# Enable automatic checkpointing
# If true, workflow state is automatically saved at regular intervals
auto_checkpoint = false

# Checkpoint interval (in steps)
# How many steps between automatic checkpoints
checkpoint_interval = 5

# Maximum number of snapshots to keep
# Older snapshots are automatically cleaned up
max_snapshots = 10

# Snapshot retention period in days
# Snapshots older than this are deleted
snapshot_retention_days = 7

# Enable workflow step debugging
# Logs detailed information about each workflow step
debug_steps = false

[a2a]
# Agent ID configuration
[a2a.agent_id]
namespace = "default"
name = "assistant"
instance = "00000000-0000-0000-0000-000000000000"

# Discovery service configuration
[a2a.discovery]
enabled = false
registry_type = "http"
registry_url = "http://localhost:8500"
heartbeat_interval = { secs = 30, nanos = 0 }
discovery_interval = { secs = 60, nanos = 0 }
ttl = { secs = 90, nanos = 0 }

# HTTP protocol configuration
[a2a.protocols.Http]
enabled = true
endpoint = "http://localhost:8080"
timeout = { secs = 30, nanos = 0 }
retry_attempts = 3
connection_pool_size = 10
settings = {}

# Security settings
[a2a.security]
enable_authentication = false
enable_encryption = false

# Routing configuration
[a2a.routing]
enable_routing = false
routing_table = {}
load_balancing = "RoundRobin"

# Circuit breaker configuration
[a2a.routing.circuit_breaker]
enabled = false
failure_threshold = 5
timeout = { secs = 60, nanos = 0 }
half_open_max_calls = 3
