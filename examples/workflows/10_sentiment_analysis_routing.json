{
  "name": "Sentiment Analysis with Conditional Routing",
  "description": "Analyzes text sentiment with LLM, routes to positive/negative responses, saves to different files",
  "nodes": [
    {
      "id": "file_input_1",
      "node_type": "file_input",
      "position": {
        "x": 50,
        "y": 200
      },
      "config": {
        "file_path": "examples/sample-data/customer_feedback.txt",
        "encoding": "utf8"
      },
      "label": "Read Feedback"
    },
    {
      "id": "sentiment_model",
      "node_type": "model_config",
      "position": {
        "x": 50,
        "y": 70
      },
      "config": {
        "provider": "ollama",
        "model": "qwen3-coder:480b-cloud",
        "base_url": "http://localhost:11434",
        "temperature": 0.2,
        "max_tokens": 50,
        "api_key": "",
        "api_version": "",
        "deployment_name": "",
        "embedding_model": "",
        "provider_options": {},
        "stream": false,
        "system_prompt": "Analyze the sentiment. Respond with only 'true' if positive or 'false' if negative.",
        "timeout": 60,
        "top_p": 0.9
      },
      "label": "Sentiment Analyzer"
    },
    {
      "id": "llm_sentiment",
      "node_type": "llm_generate",
      "position": {
        "x": 350,
        "y": 130
      },
      "config": {
        "temperature": 0.2,
        "max_tokens": 10
      },
      "label": "Analyze Sentiment"
    },
    {
      "id": "positive_response",
      "node_type": "text_input",
      "position": {
        "x": 350,
        "y": 280
      },
      "config": {
        "text": "Thank you for your positive feedback! We're glad you're satisfied."
      },
      "label": "Positive Response"
    },
    {
      "id": "negative_response",
      "node_type": "text_input",
      "position": {
        "x": 350,
        "y": 380
      },
      "config": {
        "text": "We apologize for your experience. Our team will review this feedback."
      },
      "label": "Negative Response"
    },
    {
      "id": "conditional_1",
      "node_type": "conditional",
      "position": {
        "x": 650,
        "y": 250
      },
      "config": {},
      "label": "Route by Sentiment"
    },
    {
      "id": "response_model",
      "node_type": "model_config",
      "position": {
        "x": 650,
        "y": 90
      },
      "config": {
        "provider": "ollama",
        "model": "qwen3-coder:480b-cloud",
        "base_url": "http://localhost:11434",
        "temperature": 0.7,
        "max_tokens": 300,
        "api_key": "",
        "api_version": "",
        "deployment_name": "",
        "embedding_model": "",
        "provider_options": {},
        "stream": false,
        "system_prompt": "Generate a personalized response based on the template provided.",
        "timeout": 60,
        "top_p": 0.9
      },
      "label": "Response Generator"
    },
    {
      "id": "llm_response",
      "node_type": "llm_generate",
      "position": {
        "x": 950,
        "y": 180
      },
      "config": {
        "temperature": 0.7,
        "max_tokens": 300
      },
      "label": "Generate Response"
    },
    {
      "id": "text_output_1",
      "node_type": "text_output",
      "position": {
        "x": 1250,
        "y": 190
      },
      "config": {
        "label": "Final Response",
        "format": "plain"
      },
      "label": "Display Response"
    },
    {
      "id": "file_output_1",
      "node_type": "file_output",
      "position": {
        "x": 1250,
        "y": 310
      },
      "config": {
        "file_path": "responses.txt",
        "encoding": "utf8",
        "append": true
      },
      "label": "Save Response"
    }
  ],
  "connections": [
    {
      "id": "conn_1",
      "from_node": "file_input_1",
      "from_output": "content",
      "to_node": "llm_sentiment",
      "to_input": "prompt"
    },
    {
      "id": "conn_2",
      "from_node": "sentiment_model",
      "from_output": "config",
      "to_node": "llm_sentiment",
      "to_input": "model_config"
    },
    {
      "id": "conn_3",
      "from_node": "llm_sentiment",
      "from_output": "response",
      "to_node": "conditional_1",
      "to_input": "condition"
    },
    {
      "id": "conn_4",
      "from_node": "positive_response",
      "from_output": "text",
      "to_node": "conditional_1",
      "to_input": "true_value"
    },
    {
      "id": "conn_5",
      "from_node": "negative_response",
      "from_output": "text",
      "to_node": "conditional_1",
      "to_input": "false_value"
    },
    {
      "id": "conn_6",
      "from_node": "conditional_1",
      "from_output": "output",
      "to_node": "llm_response",
      "to_input": "prompt"
    },
    {
      "id": "conn_7",
      "from_node": "response_model",
      "from_output": "config",
      "to_node": "llm_response",
      "to_input": "model_config"
    },
    {
      "id": "conn_8",
      "from_node": "llm_response",
      "from_output": "response",
      "to_node": "text_output_1",
      "to_input": "text"
    },
    {
      "id": "conn_9",
      "from_node": "llm_response",
      "from_output": "response",
      "to_node": "file_output_1",
      "to_input": "content"
    }
  ],
  "exported_at": "2025-01-08T00:00:00.000Z",
  "version": "1.0",
  "workflow_id": "00000000-0000-0000-0000-000000000010"
}