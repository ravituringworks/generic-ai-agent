{
  "name": "Multi-Provider Comparison",
  "description": "Compare responses from different LLM providers (Ollama and OpenAI) for the same prompt",
  "nodes": [
    {
      "id": "input_1",
      "node_type": "text_input",
      "position": {
        "x": 50,
        "y": 200
      },
      "config": {
        "text": "Explain quantum computing in simple terms"
      },
      "label": "Common Prompt"
    },
    {
      "id": "ollama_config",
      "node_type": "model_config",
      "position": {
        "x": 50,
        "y": 50
      },
      "config": {
        "provider": "ollama",
        "model": "qwen3-coder:480b-cloud",
        "base_url": "http://localhost:11434",
        "temperature": 0.7,
        "max_tokens": 500,
        "api_key": "",
        "api_version": "",
        "deployment_name": "",
        "embedding_model": "",
        "provider_options": {},
        "stream": false,
        "system_prompt": "",
        "timeout": 60,
        "top_p": 0.9
      },
      "label": "Ollama Config"
    },
    {
      "id": "openai_config",
      "node_type": "model_config",
      "position": {
        "x": 50,
        "y": 350
      },
      "config": {
        "provider": "openai",
        "model": "gpt-3.5-turbo",
        "base_url": "https://api.openai.com/v1",
        "temperature": 0.7,
        "max_tokens": 500,
        "api_key": "YOUR_OPENAI_API_KEY",
        "api_version": "",
        "deployment_name": "",
        "embedding_model": "",
        "provider_options": {},
        "stream": false,
        "system_prompt": "",
        "timeout": 60,
        "top_p": 0.9
      },
      "label": "OpenAI Config"
    },
    {
      "id": "llm_ollama",
      "node_type": "llm_generate",
      "position": {
        "x": 400,
        "y": 80
      },
      "config": {
        "temperature": 0.7,
        "max_tokens": 500
      },
      "label": "Ollama Response"
    },
    {
      "id": "llm_openai",
      "node_type": "llm_generate",
      "position": {
        "x": 400,
        "y": 280
      },
      "config": {
        "temperature": 0.7,
        "max_tokens": 500
      },
      "label": "OpenAI Response"
    },
    {
      "id": "output_ollama",
      "node_type": "text_output",
      "position": {
        "x": 750,
        "y": 90
      },
      "config": {
        "label": "Ollama Result",
        "format": "plain"
      },
      "label": "Ollama Output"
    },
    {
      "id": "output_openai",
      "node_type": "text_output",
      "position": {
        "x": 750,
        "y": 290
      },
      "config": {
        "label": "OpenAI Result",
        "format": "plain"
      },
      "label": "OpenAI Output"
    }
  ],
  "connections": [
    {
      "id": "conn_1",
      "from_node": "input_1",
      "from_output": "text",
      "to_node": "llm_ollama",
      "to_input": "prompt"
    },
    {
      "id": "conn_2",
      "from_node": "input_1",
      "from_output": "text",
      "to_node": "llm_openai",
      "to_input": "prompt"
    },
    {
      "id": "conn_3",
      "from_node": "ollama_config",
      "from_output": "config",
      "to_node": "llm_ollama",
      "to_input": "model_config"
    },
    {
      "id": "conn_4",
      "from_node": "openai_config",
      "from_output": "config",
      "to_node": "llm_openai",
      "to_input": "model_config"
    },
    {
      "id": "conn_5",
      "from_node": "llm_ollama",
      "from_output": "response",
      "to_node": "output_ollama",
      "to_input": "text"
    },
    {
      "id": "conn_6",
      "from_node": "llm_openai",
      "from_output": "response",
      "to_node": "output_openai",
      "to_input": "text"
    }
  ],
  "exported_at": "2025-01-08T00:00:00.000Z",
  "version": "1.0",
  "workflow_id": "00000000-0000-0000-0000-000000000005"
}